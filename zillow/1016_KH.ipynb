{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Properties ...\n",
      "Loading Train ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import neighbors\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/home/ghk829/zillow')\n",
    "print('Loading Properties ...')\n",
    "properties2016 = pd.read_csv('./properties_2016.csv', low_memory = False)\n",
    "properties2017 = pd.read_csv('./properties_2017.csv', low_memory = False)\n",
    "\n",
    "print('Loading Train ...')\n",
    "train2016 = pd.read_csv('./train_2016_v2.csv', parse_dates=['transactiondate'], low_memory=False)\n",
    "train2017 = pd.read_csv('./train_2017.csv', parse_dates=['transactiondate'], low_memory=False)\n",
    "test_df = pd.read_csv('./sample_submission.csv', low_memory=False)\n",
    "properties = pd.read_csv('./properties_2016.csv', low_memory=False)\n",
    "# field is named differently in submission\n",
    "test_df['parcelid'] = test_df['ParcelId']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_date_features(df):\n",
    "    df[\"transaction_year\"] = df[\"transactiondate\"].dt.year\n",
    "    df[\"transaction_month\"] = (df[\"transactiondate\"].dt.year - 2016)*12 + df[\"transactiondate\"].dt.month\n",
    "    df[\"transaction_day\"] = df[\"transactiondate\"].dt.day\n",
    "    df[\"transaction_quarter\"] = (df[\"transactiondate\"].dt.year - 2016)*4 +df[\"transactiondate\"].dt.quarter\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fillna_knn( df, base, target, fraction = 1, threshold = 10, n_neighbors = 5 ):\n",
    "    assert isinstance( base , list ) or isinstance( base , np.ndarray ) and isinstance( target, str ) \n",
    "    whole = [ target ] + base\n",
    "    \n",
    "    miss = df[target].isnull()\n",
    "    notmiss = ~miss \n",
    "    nummiss = miss.sum()\n",
    "    \n",
    "    enc = OneHotEncoder()\n",
    "    X_target = df.loc[ notmiss, whole ].sample( frac = fraction )\n",
    "    \n",
    "    enc.fit( X_target[ target ].unique().reshape( (-1,1) ) )\n",
    "    \n",
    "    Y = enc.transform( X_target[ target ].values.reshape((-1,1)) ).toarray()\n",
    "    X = X_target[ base  ]\n",
    "    \n",
    "    print( 'fitting' )\n",
    "    n_neighbors = n_neighbors\n",
    "    clf = neighbors.KNeighborsClassifier( n_neighbors, weights = 'uniform' )\n",
    "    clf.fit( X, Y )\n",
    "    \n",
    "    print( 'the shape of active features: ' ,enc.active_features_.shape )\n",
    "    \n",
    "    print( 'predicting' )\n",
    "    Z = clf.predict(df.loc[miss, base])\n",
    "    \n",
    "    numunperdicted = Z[:,0].sum()\n",
    "    if numunperdicted / nummiss *100 < threshold :\n",
    "        print( 'writing result to df' )    \n",
    "        df.loc[ miss, target ]  = np.dot( Z , enc.active_features_ )\n",
    "        print( 'num of unperdictable data: ', numunperdicted )\n",
    "        return enc\n",
    "    else:\n",
    "        print( 'out of threshold: {}% > {}%'.format( numunperdicted / nummiss *100 , threshold ) )\n",
    "\n",
    "#function to deal with variables that are actually string/categories\n",
    "def zoningcode2int( df, target ):\n",
    "    storenull = df[ target ].isnull()\n",
    "    enc = LabelEncoder( )\n",
    "    df[ target ] = df[ target ].astype( str )\n",
    "\n",
    "    print('fit and transform')\n",
    "    df[ target ]= enc.fit_transform( df[ target ].values )\n",
    "    print( 'num of categories: ', enc.classes_.shape  )\n",
    "    df.loc[ storenull, target ] = np.nan\n",
    "    print('recover the nan value')\n",
    "    return enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train2016 = add_date_features(train2016)\n",
    "train2017 = add_date_features(train2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge Train with Properties ...\n",
      "Tax Features 2017  ...\n",
      "Concat Train 2016 & 2017 ...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Merge Train with Properties ...')\n",
    "train2016 = pd.merge(train2016, properties2016, how = 'left', on = 'parcelid')\n",
    "train2017 = pd.merge(train2017, properties2017, how = 'left', on = 'parcelid')\n",
    "\n",
    "print('Tax Features 2017  ...')\n",
    "train2017.iloc[:, train2017.columns.str.startswith('tax')] = np.nan\n",
    "\n",
    "print('Concat Train 2016 & 2017 ...')\n",
    "train_df = pd.concat([train2016, train2017], axis = 0)\n",
    "test_df = pd.merge(test_df[['ParcelId']], properties2016.rename(columns = {'parcelid': 'ParcelId'}), how = 'left', on = 'ParcelId')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       parcelid  logerror transactiondate  transaction_year  \\\n",
      "46088  17052745    0.0980      2016-06-16              2016   \n",
      "58830  17257867   -0.0377      2016-07-22              2016   \n",
      "\n",
      "       transaction_month  transaction_day  transaction_quarter  \\\n",
      "46088                  6               16                    2   \n",
      "58830                  7               22                    3   \n",
      "\n",
      "       airconditioningtypeid  architecturalstyletypeid  basementsqft  \\\n",
      "46088                    1.0                       NaN           NaN   \n",
      "58830                    1.0                       NaN           NaN   \n",
      "\n",
      "              ...           numberofstories  fireplaceflag  \\\n",
      "46088         ...                       1.0            NaN   \n",
      "58830         ...                       1.0            NaN   \n",
      "\n",
      "       structuretaxvaluedollarcnt  taxvaluedollarcnt  assessmentyear  \\\n",
      "46088                    137697.0           433491.0          2015.0   \n",
      "58830                     58000.0           205000.0          2015.0   \n",
      "\n",
      "       landtaxvaluedollarcnt  taxamount  taxdelinquencyflag  \\\n",
      "46088               295794.0    5289.76                 NaN   \n",
      "58830               147000.0    2607.38                 NaN   \n",
      "\n",
      "       taxdelinquencyyear  censustractandblock  \n",
      "46088                 NaN         6.111001e+13  \n",
      "58830                 NaN         6.111008e+13  \n",
      "\n",
      "[2 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "dropcols = ['finishedsquarefeet12','finishedsquarefeet13', 'finishedsquarefeet15','finishedsquarefeet6']\n",
    "\n",
    "#finishedsquarefeet50 and finishedfloor1squarefeet are the exactly the same information according to the dictionary descriptions, lets remove finishedsquarefeet50 as it has more missing values\n",
    "dropcols.append('finishedsquarefeet50')\n",
    "\n",
    "#'bathroomcnt' and 'calculatedbathnbr' and 'fullbathcnt' seem to be the same information aswell according to the dictionary descriptions. Choose 'bathroomcnt' as has no missing values, so remove the other two\n",
    "dropcols.append('calculatedbathnbr')\n",
    "dropcols.append('fullbathcnt')\n",
    "\n",
    "train_df=train_df[train_df.latitude.notnull()]\n",
    "#Assume if Null in garage count it means there are no garages\n",
    "index = train_df.garagecarcnt.isnull()\n",
    "train_df.loc[index,'garagecarcnt'] = 0\n",
    "\n",
    "#Likewise no garage means the size is 0 by default\n",
    "index = train_df.garagetotalsqft.isnull()\n",
    "train_df.loc[index,'garagetotalsqft'] = 0\n",
    "\n",
    "#Let's fill in some missing values using the most common value for those variables where this might be a sensible approach\n",
    "#AC Type - Mostly 1's, which corresponds to central AC. Reasonable to assume most other properties are similar.\n",
    "train_df['airconditioningtypeid'].value_counts()\n",
    "index = train_df.airconditioningtypeid.isnull()\n",
    "train_df.loc[index,'airconditioningtypeid'] = 1\n",
    "index = train_df.heatingorsystemtypeid.isnull()\n",
    "train_df.loc[index,'heatingorsystemtypeid'] = 2\n",
    "poolsizesum_median = train_df.loc[train_df['poolcnt'] > 0, 'poolsizesum'].median()\n",
    "train_df.loc[(train_df['poolcnt'] > 0) & (train_df['poolsizesum'].isnull()), 'poolsizesum'] = poolsizesum_median\n",
    "\n",
    "#If it doesn't have a pool then poolsizesum is 0 by default\n",
    "train_df.loc[(train_df['poolcnt'] == 0), 'poolsizesum'] = 0\n",
    "train_df.loc[(train_df['finishedfloor1squarefeet'].isnull()) & (train_df['numberofstories']==1),'finishedfloor1squarefeet'] = train_df.loc[(train_df['finishedfloor1squarefeet'].isnull()) & (train_df['numberofstories']==1),'calculatedfinishedsquarefeet']\n",
    "\n",
    "#I also discovered that there seems to be two properties that have finishedfloor1squarefeet greater than calculated finishedsquarefeet. Notice also that they have big logerrors aswell - my guess is that the Zillow House price model found it difficult to predict these points due to the fact that they probably had potentially 'incorrect' data input values?\n",
    "#Discussion point - should we be removing these points or leave them in as they are or 'fix' them? I think it really depends on whether the test data has similar points which may be wrong as we'll want to predict big log errors for these incorrect points aswell I guess...\n",
    "#For now just remove them.\n",
    "print(train_df.loc[train_df['calculatedfinishedsquarefeet']<train_df['finishedfloor1squarefeet']])\n",
    "droprows = train_df.loc[train_df['calculatedfinishedsquarefeet']<train_df['finishedfloor1squarefeet']].index\n",
    "train_df = train_df.drop(droprows)\n",
    "train_df['fireplaceflag']= \"No\"\n",
    "train_df.loc[train_df['fireplacecnt']>0,'fireplaceflag']= \"Yes\"\n",
    "\n",
    "index = train_df.fireplacecnt.isnull()\n",
    "train_df.loc[index,'fireplacecnt'] = 0\n",
    "\n",
    "#Tax deliquency flag - assume if it is null then doesn't exist\n",
    "index = train_df.taxdelinquencyflag.isnull()\n",
    "train_df.loc[index,'taxdelinquencyflag'] = \"None\"\n",
    "index = train_df.threequarterbathnbr.isnull()\n",
    "train_df.loc[index,'threequarterbathnbr'] = 1\n",
    "missingvalues_prop = (train_df.isnull().sum()/len(train_df)).reset_index()\n",
    "missingvalues_prop.columns = ['field','proportion']\n",
    "missingvalues_prop = missingvalues_prop.sort_values(by = 'proportion', ascending = False)\n",
    "missingvaluescols = missingvalues_prop[missingvalues_prop['proportion'] > 0.98].field.tolist()\n",
    "dropcols = dropcols + missingvaluescols\n",
    "train_df = train_df.drop(dropcols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting\n",
      "('the shape of active features: ', (12,))\n",
      "predicting\n",
      "writing result to df\n",
      "('num of unperdictable data: ', 1903.0)\n",
      "fit and transform\n",
      "('num of categories: ', (91,))\n",
      "recover the nan value\n",
      "fitting\n",
      "('the shape of active features: ', (61,))\n",
      "predicting\n",
      "writing result to df\n",
      "('num of unperdictable data: ', 0.0)\n",
      "fit and transform\n",
      "('num of categories: ', (2347,))\n",
      "recover the nan value\n",
      "fitting\n",
      "('the shape of active features: ', (1337,))\n",
      "predicting\n",
      "writing result to df\n",
      "('num of unperdictable data: ', 0.0)\n",
      "fitting\n",
      "('the shape of active features: ', (173,))\n",
      "predicting\n",
      "writing result to df\n",
      "('num of unperdictable data: ', 2.0)\n",
      "fitting\n",
      "('the shape of active features: ', (439,))\n",
      "predicting\n",
      "writing result to df\n",
      "('num of unperdictable data: ', 406.0)\n",
      "fitting\n",
      "('the shape of active features: ', (382,))\n",
      "predicting\n",
      "writing result to df\n",
      "('num of unperdictable data: ', 0.0)\n",
      "fitting\n",
      "('the shape of active features: ', (6,))\n",
      "predicting\n",
      "out of threshold: 11.1865156308% > 10%\n",
      "fitting\n",
      "('the shape of active features: ', (123,))\n",
      "predicting\n",
      "writing result to df\n",
      "('num of unperdictable data: ', 0.0)\n",
      "fitting\n",
      "('the shape of active features: ', (10654,))\n",
      "predicting\n",
      "writing result to df\n",
      "('num of unperdictable data: ', 5.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<type 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fillna_knn( df = train_df,\n",
    "                  base = [ 'latitude', 'longitude' ] ,\n",
    "                  target = 'buildingqualitytypeid', fraction = 0.15, n_neighbors = 1 )\n",
    "\n",
    "\n",
    "zoningcode2int( df = train_df,\n",
    "                            target = 'propertycountylandusecode' )\n",
    "fillna_knn( df = train_df,\n",
    "                  base = [ 'latitude', 'longitude' ] ,\n",
    "                  target = 'propertycountylandusecode', fraction = 0.15, n_neighbors = 1 )\n",
    "\n",
    "zoningcode2int( df = train_df,\n",
    "                            target = 'propertyzoningdesc' )\n",
    "\n",
    "fillna_knn( df = train_df,\n",
    "                  base = [ 'latitude', 'longitude' ] ,\n",
    "                  target = 'propertyzoningdesc', fraction = 0.15, n_neighbors = 1 )\n",
    "\n",
    "#regionidcity, regionidneighborhood & regionidzip - assume it is the same as the nereast property. \n",
    "#As mentioned above, this is ok if there's a property very nearby to the one with missing values (I leave it up to the reader to check if this is the case!)\n",
    "fillna_knn( df = train_df,\n",
    "                  base = [ 'latitude', 'longitude' ] ,\n",
    "                  target = 'regionidcity', fraction = 0.15, n_neighbors = 1 )\n",
    "\n",
    "fillna_knn( df = train_df,\n",
    "                  base = [ 'latitude', 'longitude' ] ,\n",
    "                  target = 'regionidneighborhood', fraction = 0.15, n_neighbors = 1 )\n",
    "\n",
    "fillna_knn( df = train_df,\n",
    "                  base = [ 'latitude', 'longitude' ] ,\n",
    "                  target = 'regionidzip', fraction = 0.15, n_neighbors = 1 )\n",
    "\n",
    "#unitcnt - the number of structures the unit is built into. Assume it is the same as the nearest properties. If the property with missing values is in a block of flats or in a terrace street then this is probably ok - but again I leave it up to the reader to check if this is the case!\n",
    "fillna_knn( df = train_df,\n",
    "                  base = [ 'latitude', 'longitude' ] ,\n",
    "                  target = 'unitcnt', fraction = 0.15, n_neighbors = 1 )\n",
    "\n",
    "#yearbuilt - assume it is the same as the nearest property. This assumes properties all near to each other were built around the same time\n",
    "fillna_knn( df = train_df,\n",
    "                  base = [ 'latitude', 'longitude' ] ,\n",
    "                  target = 'yearbuilt', fraction = 0.15, n_neighbors = 1 )\n",
    "\n",
    "#lot size square feet - not sure what to do about this one. Lets use nearest neighbours. Assume it has same lot size as property closest to it\n",
    "fillna_knn( df = train_df,\n",
    "                  base = [ 'latitude', 'longitude' ] ,\n",
    "                  target = 'lotsizesquarefeet', fraction = 0.15, n_neighbors = 1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove features with one unique value !!\n",
      "We exclude: 3\n"
     ]
    }
   ],
   "source": [
    "print (\"Remove features with one unique value !!\")\n",
    "exclude_unique = []\n",
    "for c in train_df.columns:\n",
    "    num_uniques = len(train_df[c].unique())\n",
    "    if train_df[c].isnull().sum() != 0:\n",
    "        num_uniques -= 1\n",
    "    if num_uniques == 1:\n",
    "        exclude_unique.append(c)\n",
    "print(\"We exclude: %s\" % len(exclude_unique))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We exclude: []\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "missing_perc_thresh = 0.98\n",
    "exclude_missing = []\n",
    "num_rows = train_df.shape[0]\n",
    "for c in train_df.columns:\n",
    "    num_missing = train_df[c].isnull().sum()\n",
    "    if num_missing == 0:\n",
    "        continue\n",
    "    missing_frac = num_missing / float(num_rows)\n",
    "    if missing_frac > missing_perc_thresh:\n",
    "        exclude_missing.append(c)\n",
    "print(\"We exclude: %s\" % exclude_missing)\n",
    "print(len(exclude_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define training features !!\n",
      "We use these for training: ['transactiondate', 'transaction_year', 'transaction_month', 'transaction_day', 'transaction_quarter', 'airconditioningtypeid', 'bathroomcnt', 'bedroomcnt', 'buildingqualitytypeid', 'finishedfloor1squarefeet', 'calculatedfinishedsquarefeet', 'fips', 'fireplacecnt', 'garagecarcnt', 'garagetotalsqft', 'heatingorsystemtypeid', 'latitude', 'longitude', 'lotsizesquarefeet', 'poolsizesum', 'propertycountylandusecode', 'propertylandusetypeid', 'rawcensustractandblock', 'regionidcity', 'regionidcounty', 'regionidneighborhood', 'regionidzip', 'roomcnt', 'threequarterbathnbr', 'unitcnt', 'yardbuildingsqft17', 'yearbuilt', 'numberofstories', 'fireplaceflag', 'structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'assessmentyear', 'landtaxvaluedollarcnt', 'taxamount', 'taxdelinquencyflag', 'censustractandblock']\n"
     ]
    }
   ],
   "source": [
    "print (\"Define training features !!\")\n",
    "exclude_other = ['parcelid', 'logerror','propertyzoningdesc']\n",
    "train_features = []\n",
    "for c in train_df.columns:\n",
    "    if c not in exclude_missing \\\n",
    "       and c not in exclude_other and c not in exclude_unique:\n",
    "        train_features.append(c)\n",
    "print(\"We use these for training: %s\" % train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing NaN values by -999 !!\n",
      "remove outliers\n",
      "0:finishedfloor1squarefeet\n",
      "1:garagetotalsqft\n",
      "2:lotsizesquarefeet\n",
      "3:poolsizesum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghk829/miniconda2/lib/python2.7/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ghk829/miniconda2/lib/python2.7/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train: ', (74476, 51))\n",
      "('Test: ', (89969, 51))\n"
     ]
    }
   ],
   "source": [
    "print (\"Replacing NaN values by -999 !!\")\n",
    "train_df.fillna(-999, inplace=True)\n",
    "test_df.fillna(-999, inplace=True)\n",
    "\n",
    "\n",
    "print (\"remove outliers\")\n",
    "train_df=train_df[ train_df.logerror > -0.4 ]\n",
    "train_df=train_df[ train_df.logerror < 0.419 ]\n",
    "\n",
    "train_df=train_df.assign(diff_cal_fin=lambda x: x.calculatedfinishedsquarefeet-x.finishedfloor1squarefeet)\n",
    "new_cols=['finishedfloor1squarefeet',\n",
    "'garagetotalsqft',\n",
    "'lotsizesquarefeet',\n",
    "'poolsizesum']\n",
    "for i,value in enumerate(new_cols):\n",
    "    print(str(i)+\":\"+value)\n",
    "    train_df=eval(\"train_df.assign(new_{}=lambda x: x.{} /x.calculatedfinishedsquarefeet)\".format(value,value))\n",
    "\n",
    "VAL_SPLIT_DATE = '2016-09-15'   # Cutoff date for validation split\n",
    "select_qtr4 = train_df[\"transactiondate\"] >= VAL_SPLIT_DATE\n",
    "valid = train_df[select_qtr4]\n",
    "train = train_df[~select_qtr4]\n",
    "valid.drop([\"transactiondate\"], inplace=True, axis=1)\n",
    "train.drop([\"transactiondate\"], inplace=True, axis=1)\n",
    "print(\"Train: \", train.shape)\n",
    "print(\"Test: \", valid.shape)\n",
    "\n",
    "test_df['transactiondate'] = pd.Timestamp('2016-12-01') \n",
    "test_df = add_date_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Define categorial features !!\n",
      "Cat features are: ['transaction_year', 'transaction_month', 'transaction_day', 'transaction_quarter', 'airconditioningtypeid', 'buildingqualitytypeid', 'fips', 'heatingorsystemtypeid', 'poolsizesum', 'propertycountylandusecode', 'propertylandusetypeid', 'regionidcity', 'regionidcounty', 'regionidneighborhood', 'regionidzip', 'yearbuilt', 'fireplaceflag', 'assessmentyear', 'taxdelinquencyflag']\n"
     ]
    }
   ],
   "source": [
    "train_features.remove('transactiondate')\n",
    "print (\"Define categorial features !!\")\n",
    "cat_feature_inds = []\n",
    "cat_unique_thresh = 1000\n",
    "for i, c in enumerate(train_features):\n",
    "    num_uniques = len(train_df[c].unique())\n",
    "    if num_uniques < cat_unique_thresh \\\n",
    "       and not 'sqft' in c \\\n",
    "       and not 'cnt' in c \\\n",
    "       and not 'nbr' in c \\\n",
    "       and not 'number' in c:\n",
    "        cat_feature_inds.append(i)\n",
    "        \n",
    "print(\"Cat features are: %s\" % [train_features[ind] for ind in cat_feature_inds])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features_new = train_features+['new_finishedfloor1squarefeet',\n",
    "'new_garagetotalsqft',\n",
    "'new_lotsizesquarefeet',\n",
    "'new_poolsizesum','diff_cal_fin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((74476, 45), (74476,))\n",
      "((89969, 45), (89969,))\n"
     ]
    }
   ],
   "source": [
    "X_train = train[train_features_new]\n",
    "y_train = train.logerror\n",
    "X_valid = valid[train_features_new]\n",
    "y_valid = valid.logerror\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [03:59<00:00, 239.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.052304288207313684]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_ensembles = 1\n",
    "tree_counts = []\n",
    "MAEs = []\n",
    "for i in tqdm(range(num_ensembles)):\n",
    "    # TODO(you): Use CV, tune hyperparameters\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=630, learning_rate=0.003,\n",
    "        depth=6, l2_leaf_reg=3,\n",
    "        bagging_temperature=8,\n",
    "        loss_function='MAE',\n",
    "        eval_metric='MAE',\n",
    "        random_seed=i)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[X_valid, y_valid],\n",
    "        cat_features=cat_feature_inds,\n",
    "#        verbose=True,\n",
    "        use_best_model=True\n",
    "        )\n",
    "    tree_counts.append( model.tree_count_ )\n",
    "    MAEs.append( mean_absolute_error(y_valid, model.predict(X_valid)) )\n",
    "print(MAEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [20:06<00:00, 241.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.052294549989204735, 0.052302874108787568, 0.052329831216809314, 0.052240020286305588, 0.052322335282048001]\n"
     ]
    }
   ],
   "source": [
    "num_ensembles = 5\n",
    "tree_counts = []\n",
    "MAEs = []\n",
    "for i in tqdm(range(num_ensembles)):\n",
    "    # TODO(you): Use CV, tune hyperparameters\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=630, learning_rate=0.003,\n",
    "        depth=6, l2_leaf_reg=3,\n",
    "        bagging_temperature=8,\n",
    "        loss_function='MAE',\n",
    "        eval_metric='MAE',\n",
    "        random_seed=i)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[X_valid, y_valid],\n",
    "        cat_features=cat_feature_inds,\n",
    "#        verbose=True,\n",
    "        use_best_model=True\n",
    "        )\n",
    "    tree_counts.append( model.tree_count_ )\n",
    "    MAEs.append( mean_absolute_error(y_valid, model.predict(X_valid)) )\n",
    "print(MAEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 중요도가 2 이상인 features만 가져오기 (선택) 둘 다 돌려보고 더 좋은 걸로 ...\n",
    "tmp=[]\n",
    "for idx, value in enumerate([i>2 for i in model.feature_importances_]):\n",
    "    if value==True:\n",
    "        tmp.append(idx)\n",
    "train_featured=[train_features_new[i] for i in tmp]\n",
    "X_train = train[train_featured]\n",
    "y_train = train.logerror\n",
    "X_valid = valid[train_featured]\n",
    "y_valid = valid.logerror\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_valid.shape, y_valid.shape)\n",
    "print (\"Define categorial features !!\")\n",
    "cat_feature_inds = []\n",
    "cat_unique_thresh = 1000\n",
    "for i, c in enumerate(train_featured):\n",
    "    num_uniques = len(train_df[c].unique())\n",
    "    if num_uniques < cat_unique_thresh \\\n",
    "       and not 'sqft' in c \\\n",
    "       and not 'cnt' in c \\\n",
    "       and not 'nbr' in c \\\n",
    "       and not 'number' in c:\n",
    "        cat_feature_inds.append(i)\n",
    "        \n",
    "print(\"Cat features are: %s\" % [train_features[ind] for ind in cat_feature_inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df=test_df.assign(diff_cal_fin=lambda x: x.calculatedfinishedsquarefeet-x.finishedfloor1squarefeet)\n",
    "new_cols=['finishedfloor1squarefeet',\n",
    "'garagetotalsqft',\n",
    "'lotsizesquarefeet',\n",
    "'poolsizesum']\n",
    "for i,value in enumerate(new_cols):\n",
    "    print(str(i)+\":\"+value)\n",
    "    test_df=eval(\"test_df.assign(new_{}=lambda x: x.{} /x.calculatedfinishedsquarefeet)\".format(value,value))\n",
    "X_test=test_df[train_features_new]\n",
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for: 201612 ... \n",
      "Predicting for: 201610 ... \n",
      "Predicting for: 201611 ... \n",
      "Predicting for: 201712 ... \n",
      "Predicting for: 201711 ... \n",
      "Predicting for: 201710 ... \n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'ParcelId': test_df['ParcelId'],\n",
    "})\n",
    "test_dates = {\n",
    "    '201610': pd.Timestamp('2016-10-01'),\n",
    "    '201611': pd.Timestamp('2016-11-01'),\n",
    "    '201612': pd.Timestamp('2016-12-01'),\n",
    "    '201710': pd.Timestamp('2017-10-01'),\n",
    "    '201711': pd.Timestamp('2017-11-01'),\n",
    "    '201712': pd.Timestamp('2017-12-02')\n",
    "}\n",
    "for label, test_date in test_dates.items():\n",
    "    print(\"Predicting for: %s ... \" % (label))\n",
    "    submission[label] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove missing data fields ...\n",
      "We exclude: 15\n",
      "fitting\n",
      "('the shape of active features: ', (6,))\n",
      "predicting\n",
      "writing result to df\n",
      "('num of unperdictable data: ', 221.0)\n",
      "fit and transform\n",
      "('num of categories: ', (78,))\n",
      "recover the nan value\n",
      "fitting\n",
      "('the shape of active features: ', (46,))\n",
      "predicting\n",
      "writing result to df\n",
      "('num of unperdictable data: ', 0.0)\n",
      "fit and transform\n",
      "('num of categories: ', (1997,))\n",
      "recover the nan value\n",
      "fitting\n",
      "('the shape of active features: ', (1075,))\n",
      "predicting\n",
      "writing result to df\n",
      "('num of unperdictable data: ', 0.0)\n",
      "fitting\n",
      "('the shape of active features: ', (170,))\n",
      "predicting\n",
      "writing result to df\n",
      "('num of unperdictable data: ', 2.0)\n",
      "fitting\n",
      "('the shape of active features: ', (380,))\n",
      "predicting\n",
      "writing result to df\n",
      "('num of unperdictable data: ', 222.0)\n",
      "fitting\n",
      "('the shape of active features: ', (378,))\n",
      "predicting\n",
      "writing result to df\n",
      "('num of unperdictable data: ', 0.0)\n",
      "fitting\n",
      "('the shape of active features: ', (4,))\n",
      "predicting\n",
      "out of threshold: 18.4512248606% > 10%\n",
      "fitting\n",
      "('the shape of active features: ', (124,))\n",
      "predicting\n",
      "writing result to df\n",
      "('num of unperdictable data: ', 0.0)\n",
      "fitting\n",
      "('the shape of active features: ', (7126,))\n",
      "predicting\n",
      "writing result to df\n",
      "('num of unperdictable data: ', 0.0)\n",
      "Remove features with one unique value !!\n",
      "We exclude: 12\n",
      "Define training features !!\n",
      "We use these for training: 41\n",
      "Replacing NaN values by 0 !!\n",
      "remove outliers\n",
      "0:finishedfloor1squarefeet\n",
      "1:garagetotalsqft\n",
      "2:lotsizesquarefeet\n",
      "3:poolsizesum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghk829/miniconda2/lib/python2.7/site-packages/ipykernel_launcher.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/ghk829/miniconda2/lib/python2.7/site-packages/ipykernel_launcher.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train: ', (74478, 68))\n",
      "('Test: ', (14050, 68))\n",
      "Define categorial features !!\n",
      "Cat features are: ['transaction_month', 'transaction_day', 'transaction_quarter', 'airconditioningtypeid', 'buildingqualitytypeid', 'fips', 'heatingorsystemtypeid', 'propertycountylandusecode', 'propertylandusetypeid', 'regionidcity', 'regionidcounty', 'regionidneighborhood', 'regionidzip', 'yearbuilt']\n",
      "['transaction_month', 'transaction_day', 'transaction_quarter', 'airconditioningtypeid', 'bathroomcnt', 'bedroomcnt', 'buildingqualitytypeid', 'calculatedbathnbr', 'finishedfloor1squarefeet', 'calculatedfinishedsquarefeet', 'finishedsquarefeet12', 'finishedsquarefeet15', 'finishedsquarefeet50', 'fips', 'fireplacecnt', 'fullbathcnt', 'garagecarcnt', 'garagetotalsqft', 'heatingorsystemtypeid', 'latitude', 'longitude', 'lotsizesquarefeet', 'propertycountylandusecode', 'propertylandusetypeid', 'rawcensustractandblock', 'regionidcity', 'regionidcounty', 'regionidneighborhood', 'regionidzip', 'roomcnt', 'threequarterbathnbr', 'unitcnt', 'yardbuildingsqft17', 'yearbuilt', 'numberofstories', 'structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxamount', 'censustractandblock', 'new_finishedfloor1squarefeet', 'new_garagetotalsqft', 'new_lotsizesquarefeet', 'new_poolsizesum', 'diff_cal_fin']\n",
      "((74478, 45), (74478,))\n",
      "((14050, 45), (14050,))\n"
     ]
    }
   ],
   "source": [
    "train_df=train2016\n",
    "print('Remove missing data fields ...')\n",
    "\n",
    "missing_perc_thresh = 0.98\n",
    "exclude_missing = []\n",
    "num_rows = train_df.shape[0]\n",
    "for c in train_df.columns:\n",
    "    num_missing = train_df[c].isnull().sum()\n",
    "    if num_missing == 0:\n",
    "        continue\n",
    "    missing_frac = num_missing / float(num_rows)\n",
    "    if missing_frac > missing_perc_thresh:\n",
    "        exclude_missing.append(c)\n",
    "print(\"We exclude: %s\" % len(exclude_missing))\n",
    "\n",
    "del num_rows, missing_perc_thresh\n",
    "gc.collect();\n",
    "dropcols = ['finishedsquarefeet12','finishedsquarefeet13', 'finishedsquarefeet15','finishedsquarefeet6']\n",
    "\n",
    "#finishedsquarefeet50 and finishedfloor1squarefeet are the exactly the same information according to the dictionary descriptions, lets remove finishedsquarefeet50 as it has more missing values\n",
    "dropcols.append('finishedsquarefeet50')\n",
    "\n",
    "#'bathroomcnt' and 'calculatedbathnbr' and 'fullbathcnt' seem to be the same information aswell according to the dictionary descriptions. Choose 'bathroomcnt' as has no missing values, so remove the other two\n",
    "dropcols.append('calculatedbathnbr')\n",
    "dropcols.append('fullbathcnt')\n",
    "\n",
    "train_df=train_df[train_df.latitude.notnull()]\n",
    "#Assume if Null in garage count it means there are no garages\n",
    "index = train_df.garagecarcnt.isnull()\n",
    "train_df.loc[index,'garagecarcnt'] = 0\n",
    "\n",
    "#Likewise no garage means the size is 0 by default\n",
    "index = train_df.garagetotalsqft.isnull()\n",
    "train_df.loc[index,'garagetotalsqft'] = 0\n",
    "\n",
    "#Let's fill in some missing values using the most common value for those variables where this might be a sensible approach\n",
    "#AC Type - Mostly 1's, which corresponds to central AC. Reasonable to assume most other properties are similar.\n",
    "train_df['airconditioningtypeid'].value_counts()\n",
    "index = train_df.airconditioningtypeid.isnull()\n",
    "train_df.loc[index,'airconditioningtypeid'] = 1\n",
    "index = train_df.heatingorsystemtypeid.isnull()\n",
    "train_df.loc[index,'heatingorsystemtypeid'] = 2\n",
    "poolsizesum_median = train_df.loc[train_df['poolcnt'] > 0, 'poolsizesum'].median()\n",
    "train_df.loc[(train_df['poolcnt'] > 0) & (train_df['poolsizesum'].isnull()), 'poolsizesum'] = poolsizesum_median\n",
    "\n",
    "#If it doesn't have a pool then poolsizesum is 0 by default\n",
    "train_df.loc[(train_df['poolcnt'] == 0), 'poolsizesum'] = 0\n",
    "train_df.loc[(train_df['finishedfloor1squarefeet'].isnull()) & (train_df['numberofstories']==1),'finishedfloor1squarefeet'] = train_df.loc[(train_df['finishedfloor1squarefeet'].isnull()) & (train_df['numberofstories']==1),'calculatedfinishedsquarefeet']\n",
    "\n",
    "#I also discovered that there seems to be two properties that have finishedfloor1squarefeet greater than calculated finishedsquarefeet. Notice also that they have big logerrors aswell - my guess is that the Zillow House price model found it difficult to predict these points due to the fact that they probably had potentially 'incorrect' data input values?\n",
    "#Discussion point - should we be removing these points or leave them in as they are or 'fix' them? I think it really depends on whether the test data has similar points which may be wrong as we'll want to predict big log errors for these incorrect points aswell I guess...\n",
    "#For now just remove them.\n",
    "print(train_df.loc[train_df['calculatedfinishedsquarefeet']<train_df['finishedfloor1squarefeet']])\n",
    "droprows = train_df.loc[train_df['calculatedfinishedsquarefeet']<train_df['finishedfloor1squarefeet']].index\n",
    "train_df = train_df.drop(droprows)\n",
    "train_df['fireplaceflag']= \"No\"\n",
    "train_df.loc[train_df['fireplacecnt']>0,'fireplaceflag']= \"Yes\"\n",
    "\n",
    "index = train_df.fireplacecnt.isnull()\n",
    "train_df.loc[index,'fireplacecnt'] = 0\n",
    "\n",
    "#Tax deliquency flag - assume if it is null then doesn't exist\n",
    "index = train_df.taxdelinquencyflag.isnull()\n",
    "train_df.loc[index,'taxdelinquencyflag'] = \"None\"\n",
    "index = train_df.threequarterbathnbr.isnull()\n",
    "train_df.loc[index,'threequarterbathnbr'] = 1\n",
    "missingvalues_prop = (train_df.isnull().sum()/len(train_df)).reset_index()\n",
    "missingvalues_prop.columns = ['field','proportion']\n",
    "missingvalues_prop = missingvalues_prop.sort_values(by = 'proportion', ascending = False)\n",
    "missingvaluescols = missingvalues_prop[missingvalues_prop['proportion'] > 0.98].field.tolist()\n",
    "dropcols = dropcols + missingvaluescols\n",
    "train_df = train_df.drop(dropcols, axis=1)\n",
    "#Assume if Null in garage count it means there are no garages\n",
    "index = train_df.garagecarcnt.isnull()\n",
    "train_df.loc[index,'garagecarcnt'] = 0\n",
    "\n",
    "#Likewise no garage means the size is 0 by default\n",
    "index = train_df.garagetotalsqft.isnull()\n",
    "train_df.loc[index,'garagetotalsqft'] = 0\n",
    "\n",
    "#Let's fill in some missing values using the most common value for those variables where this might be a sensible approach\n",
    "#AC Type - Mostly 1's, which corresponds to central AC. Reasonable to assume most other properties are similar.\n",
    "train_df['airconditioningtypeid'].value_counts()\n",
    "index = train_df.airconditioningtypeid.isnull()\n",
    "train_df.loc[index,'airconditioningtypeid'] = 1\n",
    "index = train_df.heatingorsystemtypeid.isnull()\n",
    "train_df.loc[index,'heatingorsystemtypeid'] = 2\n",
    "fillna_knn( df = train_df,\n",
    "                  base = [ 'latitude', 'longitude' ] ,\n",
    "                  target = 'buildingqualitytypeid', fraction = 0.15, n_neighbors = 1 )\n",
    "\n",
    "\n",
    "zoningcode2int( df = train_df,\n",
    "                            target = 'propertycountylandusecode' )\n",
    "fillna_knn( df = train_df,\n",
    "                  base = [ 'latitude', 'longitude' ] ,\n",
    "                  target = 'propertycountylandusecode', fraction = 0.15, n_neighbors = 1 )\n",
    "\n",
    "zoningcode2int( df = train_df,\n",
    "                            target = 'propertyzoningdesc' )\n",
    "\n",
    "fillna_knn( df = train_df,\n",
    "                  base = [ 'latitude', 'longitude' ] ,\n",
    "                  target = 'propertyzoningdesc', fraction = 0.15, n_neighbors = 1 )\n",
    "\n",
    "#regionidcity, regionidneighborhood & regionidzip - assume it is the same as the nereast property. \n",
    "#As mentioned above, this is ok if there's a property very nearby to the one with missing values (I leave it up to the reader to check if this is the case!)\n",
    "fillna_knn( df = train_df,\n",
    "                  base = [ 'latitude', 'longitude' ] ,\n",
    "                  target = 'regionidcity', fraction = 0.15, n_neighbors = 1 )\n",
    "\n",
    "fillna_knn( df = train_df,\n",
    "                  base = [ 'latitude', 'longitude' ] ,\n",
    "                  target = 'regionidneighborhood', fraction = 0.15, n_neighbors = 1 )\n",
    "\n",
    "fillna_knn( df = train_df,\n",
    "                  base = [ 'latitude', 'longitude' ] ,\n",
    "                  target = 'regionidzip', fraction = 0.15, n_neighbors = 1 )\n",
    "\n",
    "#unitcnt - the number of structures the unit is built into. Assume it is the same as the nearest properties. If the property with missing values is in a block of flats or in a terrace street then this is probably ok - but again I leave it up to the reader to check if this is the case!\n",
    "fillna_knn( df = train_df,\n",
    "                  base = [ 'latitude', 'longitude' ] ,\n",
    "                  target = 'unitcnt', fraction = 0.15, n_neighbors = 1 )\n",
    "\n",
    "#yearbuilt - assume it is the same as the nearest property. This assumes properties all near to each other were built around the same time\n",
    "fillna_knn( df = train_df,\n",
    "                  base = [ 'latitude', 'longitude' ] ,\n",
    "                  target = 'yearbuilt', fraction = 0.15, n_neighbors = 1 )\n",
    "\n",
    "#lot size square feet - not sure what to do about this one. Lets use nearest neighbours. Assume it has same lot size as property closest to it\n",
    "fillna_knn( df = train_df,\n",
    "                  base = [ 'latitude', 'longitude' ] ,\n",
    "                  target = 'lotsizesquarefeet', fraction = 0.15, n_neighbors = 1 )\n",
    "\n",
    "print (\"Remove features with one unique value !!\")\n",
    "exclude_unique = []\n",
    "for c in train_df.columns:\n",
    "    num_uniques = len(train_df[c].unique())\n",
    "    if train_df[c].isnull().sum() != 0:\n",
    "        num_uniques -= 1\n",
    "    if num_uniques == 1:\n",
    "        exclude_unique.append(c)\n",
    "print(\"We exclude: %s\" % len(exclude_unique))\n",
    "print (\"Define training features !!\")\n",
    "exclude_other = ['parcelid', 'logerror','propertyzoningdesc']\n",
    "train_features = []\n",
    "for c in train_df.columns:\n",
    "    if c not in exclude_missing \\\n",
    "       and c not in exclude_other and c not in exclude_unique:\n",
    "        train_features.append(c)\n",
    "print(\"We use these for training: %s\" % len(train_features))\n",
    "print (\"Replacing NaN values by -999 !!\")\n",
    "train_df.fillna(-999, inplace=True)\n",
    "test_df.fillna(-999, inplace=True)\n",
    "\n",
    "\n",
    "print (\"remove outliers\")\n",
    "train_df=train_df[ train_df.logerror > -0.4 ]\n",
    "train_df=train_df[ train_df.logerror < 0.419 ]\n",
    "\n",
    "train_df=train_df.assign(diff_cal_fin=lambda x: x.calculatedfinishedsquarefeet-x.finishedfloor1squarefeet)\n",
    "new_cols=['finishedfloor1squarefeet',\n",
    "'garagetotalsqft',\n",
    "'lotsizesquarefeet',\n",
    "'poolsizesum']\n",
    "for i,value in enumerate(new_cols):\n",
    "    print(str(i)+\":\"+value)\n",
    "    train_df=eval(\"train_df.assign(new_{}=lambda x: x.{} /x.calculatedfinishedsquarefeet)\".format(value,value))\n",
    "\n",
    "VAL_SPLIT_DATE = '2016-09-15'   # Cutoff date for validation split\n",
    "select_qtr4 = train_df[\"transactiondate\"] >= VAL_SPLIT_DATE\n",
    "valid = train_df[select_qtr4]\n",
    "train = train_df[~select_qtr4]\n",
    "valid.drop([\"transactiondate\"], inplace=True, axis=1)\n",
    "train.drop([\"transactiondate\"], inplace=True, axis=1)\n",
    "print(\"Train: \", train.shape)\n",
    "print(\"Test: \", valid.shape)\n",
    "\n",
    "test_df['transactiondate'] = pd.Timestamp('2016-12-01') \n",
    "test_df = add_date_features(test_df)\n",
    "train_features.remove('transactiondate')\n",
    "print (\"Define categorial features !!\")\n",
    "cat_feature_inds = []\n",
    "cat_unique_thresh = 1000\n",
    "for i, c in enumerate(train_features):\n",
    "    num_uniques = len(train_df[c].unique())\n",
    "    if num_uniques < cat_unique_thresh \\\n",
    "       and not 'sqft' in c \\\n",
    "       and not 'cnt' in c \\\n",
    "       and not 'nbr' in c \\\n",
    "       and not 'number' in c:\n",
    "        cat_feature_inds.append(i)\n",
    "        \n",
    "print(\"Cat features are: %s\" % [train_features[ind] for ind in cat_feature_inds])\n",
    "train_features_new = train_features+['new_finishedfloor1squarefeet',\n",
    "'new_garagetotalsqft',\n",
    "'new_lotsizesquarefeet',\n",
    "'new_poolsizesum','diff_cal_fin']\n",
    "print train_features_new\n",
    "X_train = train[train_features_new]\n",
    "y_train = train.logerror\n",
    "X_valid = valid[train_features_new]\n",
    "y_valid = valid.logerror\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.051815088623889873, 0.051819626203590659, 0.051794808012161873]\n"
     ]
    }
   ],
   "source": [
    "num_ensembles = 5\n",
    "tree_counts = []\n",
    "MAEs = []\n",
    "for i in tqdm(range(num_ensembles)):\n",
    "    # TODO(you): Use CV, tune hyperparameters\n",
    "    model2 = CatBoostRegressor(\n",
    "        iterations=630, learning_rate=0.003,\n",
    "        depth=6, l2_leaf_reg=3,\n",
    "        bagging_temperature=8,\n",
    "        loss_function='MAE',\n",
    "        eval_metric='MAE',\n",
    "        random_seed=i)\n",
    "    model2.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[X_valid, y_valid],\n",
    "        cat_features=cat_feature_inds,\n",
    "#        verbose=True,\n",
    "        use_best_model=True\n",
    "        )\n",
    "    tree_counts.append( model2.tree_count_ )\n",
    "    MAEs.append( mean_absolute_error(y_valid, model2.predict(X_valid)) )\n",
    "print(MAEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:finishedfloor1squarefeet\n",
      "1:garagetotalsqft\n",
      "2:lotsizesquarefeet\n",
      "3:poolsizesum\n"
     ]
    }
   ],
   "source": [
    "test_df=test_df.assign(diff_cal_fin=lambda x: x.calculatedfinishedsquarefeet-x.finishedfloor1squarefeet)\n",
    "new_cols=['finishedfloor1squarefeet',\n",
    "'garagetotalsqft',\n",
    "'lotsizesquarefeet',\n",
    "'poolsizesum']\n",
    "for i,value in enumerate(new_cols):\n",
    "    print(str(i)+\":\"+value)\n",
    "    test_df=eval(\"test_df.assign(new_{}=lambda x: x.{} /x.calculatedfinishedsquarefeet)\".format(value,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test=test_df[train_features_new]\n",
    "y_pred=model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for: 201612 ... \n",
      "Predicting for: 201610 ... \n",
      "Predicting for: 201611 ... \n"
     ]
    }
   ],
   "source": [
    "test_dates = {\n",
    "    '201610': pd.Timestamp('2016-10-01'),\n",
    "    '201611': pd.Timestamp('2016-11-01'),\n",
    "    '201612': pd.Timestamp('2016-12-01')\n",
    "}\n",
    "for label, test_date in test_dates.items():\n",
    "    print(\"Predicting for: %s ... \" % (label))\n",
    "    submission[label] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('./final_solution_0.csv', float_format='%.6f',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
